{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи и подходы кластеризации\n",
    "\n",
    "**Задача кластеризации**: найти отображение множества входных объектов , которое разделило бы множество  на подгруппы. Кластеризация — это обучение **без учителя**.\n",
    "\n",
    "Формальная запись задачи кластеризации выглядит следующим образом:\n",
    "\n",
    "Пусть $X$ — множество объектов, $Y$  — множество **метод кластеров** (идентификаторов их принадлежности). На множестве $X$ задана функция, которая вычисляет расстояние между объектами: \n",
    "$$\\rho\\left(x, x^{\\prime}\\right)$$. \n",
    "\n",
    "Также дана конечная обучающая выборка объектов:\n",
    "$$X^{m}=\\left\\{x_{1}, \\ldots, x_{m}\\right\\} \\subset X$$ \n",
    "\n",
    "Нам надо разбить выборку на кластеры, то есть поставить каждому объекту $x_{i} \\in X^{m}$ в соответствие метку $y_{i} \\in Y$ так, чтобы внутри каждого кластера объекты были **как можно более близки** (то есть расстояние должно быть минимальным), а объекты из разных кластеров значительно различались.\n",
    "\n",
    "В задаче кластеризации входные данные задаются двумя способами:\n",
    " - Признаковое описание объектов: все объекты описываются некоторыми характеристиками (значениями признаков)\n",
    " - Матрица расстояний между объектами: для каждого объекта представлены расстояния от него до всех остальных объектов выборки\n",
    " \n",
    "**Некорректность задачи кластеризации** — решение задачи кластеризации принципиально неоднозначно:\n",
    " - Нет **точной постановки** задачи кластеризации.\n",
    " - Существует **множество критериев** качества кластеризации.\n",
    " - Существует **множество методов** кластеризации.\n",
    " - Часто заранее **неизвестно число** кластеров.\n",
    " - Результат кластеризации зависит от метрики, которая задаётся субъективно.\n",
    "\n",
    "**Для чего нужны разные подходы кластеризации?**\n",
    "\n",
    "1. Разные цели кластеризации:\n",
    "    - Упростить дальнейшую обработку данных: разбить множество объектов на несколько групп (кластеров), чтобы в дальнейшем работать с каждым кластером в отдельности\n",
    "    - Сократить объём хранимых данных: выделить кластеры и оставить по одному объекту от каждого кластера и таким образом сжать данные\n",
    "    - Выделить нетипичные объекты: выделить объекты, которые нельзя отнести ни к одному из кластеров\n",
    "    - Построить иерархию множества объектов: задача таксономии.\n",
    "2. Разнообразие условий задач кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условия задач кластеризации\n",
    "\n",
    "\n",
    "1. **Форма кластеров**: внутрикластерные расстояния меньше межкластерных, ленточная структура, кластеры с центром, кластеры соединены перемычками, разреженный фон, пересекающиеся кластеры, кластеры отсутствуют, кластеры образуются не по близости расстояний.\n",
    "\n",
    "<img src=\"../images/m4.2_1.png\" alt=\"Binary-cross-entropy\" width=\"900\" align=\"center\">\n",
    "\n",
    "2. **Вложенность кластеров друг в друга**.\n",
    "3. **Размер кластеров**: один кластер — одна тема, один кластер — одно большое событие, один кластер — одна новость.\n",
    "4. **Кластеризация как основная или вспомогательная задача**.\n",
    "5. **Жёсткая** (определяем конкретный кластер для объекта) или **мягкая** (определяем вероятность принадлежности объекта к кластеру) **кластеризация**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы кластеризации\n",
    "\n",
    "Рассмотрим четыре основных алгоритма кластеризации:\n",
    " - k-means;\n",
    " - EM-алгоритм;\n",
    " - DBSCAN;\n",
    " - агломеративная кластеризация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS\n",
    "\n",
    "**k-means** является одним из самых популярных и простых алгоритмов кластеризации. В ходе этого алгоритма все элементы пространства разбиваются на заранее известное (заданное) количество кластеров. Суть алгоритма состоит в том, что он пытается оптимизировать расстояние всех точек кластера от центра этого кластера. То есть он формирует кластеры так, чтобы внутри каждого сумма квадратов расстояний от точек до центра кластера была как можно меньше.\n",
    "\n",
    "**Схема действия алгоритма k-means**\n",
    " - Выбрать количество кластеров, которое нам кажется оптимальным для наших данных.\n",
    " - Выбрать случайным образом в пространство наших данных центроиды.\n",
    " - Для каждой точки набора данных посчитать, к какому центроиду она ближе.\n",
    " - Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду. Каждый центроид на каждой итерации — вектор, элементы которого представляют собой средние значения признаков, вычисленные по всем записям кластера.\n",
    " - Повторять шаги 3-4 фиксированное число раз или до тех пор, пока центроиды не сойдутся.\n",
    " \n",
    "Итерации алгоритма выглядят визуально следующим образом:\n",
    "\n",
    "<img src=\"../images/ml-4-4-3.png\" alt=\"Binary-cross-entropy\" width=\"900\" align=\"center\">\n",
    "\n",
    "У этого алгоритма есть ряд недостатков:\n",
    " - число кластеров надо знать заранее;\n",
    " - алгоритм очень чувствителен к первичному выбору центроидов;\n",
    " - не гарантирует достижение глобального минимума суммы квадратов расстояний, часто «застревает» в локальном минимуме.\n",
    " - У данного алгоритма есть также вариации, которые применяются в некоторых специфических случаях. Рассмотрим их ниже.\n",
    " \n",
    "## Mini-Batch K-means\n",
    "\n",
    "Данная вариация k-means используется в случае, если данных очень много. Из-за объема данных вычисление центров по всей выборке происходит долго. Решение проблемы: на каждом шаге k-means работать с небольшой **подвыборкой данных**. В общем случае упрощённый алгоритм должен сходиться к тому же результату, что и на полной выборке. Однако исследования показывают, что качество кластеров может ухудшаться по сравнению с классическим k-means.\n",
    "\n",
    "## K-means++\n",
    "\n",
    "Ещё одну вариацию алгоритма k-means мы используем в том случае, если у нас очень много признаков. Как известно, результат и время работы k-means зависит от изначального выбора центров. Чтобы минимизировать затраты, мы будем действовать следующим образом:\n",
    " - Первый центр выбираем случайно из равномерного распределения на выборке.\n",
    " - Каждый следующий центр выбираем случайно из оставшихся точек так, чтобы вероятность выбрать каждую точку была пропорциональна квадрату расстояния от неё до ближайшего центра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-АЛГОРИТМ\n",
    "\n",
    "Следующий алгоритм кластеризации — EM-алгоритм. Последовательность действий в нём выглядит следующим образом:\n",
    "\n",
    "**Схема действий EM-алгоритма**\n",
    " - Выбрать количество кластеров, которое нам кажется **оптимальным** для наших данных.\n",
    " - Выбрать случайным образом в пространство наших данных **параметры распределений**.\n",
    " - Для каждой точки нашего набора данных посчитать **вероятность принадлежности** к каждому кластеру.\n",
    " - Обновить параметры распределений таким образом, чтобы **максимизировать** вероятность принадлежности точек, отнесённых к кластеру.\n",
    " - Повторять шаги 3-4 фиксированное число раз, либо до тех пор пока центроиды не сойдутся.\n",
    " \n",
    "**ПРИМЕР**\n",
    "\n",
    "**Дано**:\n",
    " - $k$ кластеров.\n",
    " - Априорные вероятности кластеров: $w_1, ......, w_k$\n",
    " - Плотности распределения кластеров: $p_1(x), ......, p_k(x)$\n",
    "\n",
    "**Определить**:\n",
    "Плотность распределения признаков объекта $x$: $p(x)=\\sum_{j=1}^{k}w_jp_j(x)$\n",
    "\n",
    "**Результат**:\n",
    "Получим вероятность принадлежности объекта к кластеру.\n",
    "$$p(x)=\\sum_{j=1}^{k}w_jp_j(x)$$\n",
    "$$p_j(x)=\\phi_j(\\theta _j;x)$$\n",
    "\n",
    "**E-шаг (expectation)** — вычисляем ожидаемый кластер для каждого объекта:\n",
    "$$g_{ji}=p(j|x_i)=\\frac{w_jp_j(x_i)}{p(x_i)}$$\n",
    "\n",
    "**M-шаг (maximization)** — оцениваем вес и параметры распределения для каждого кластера:\n",
    "$$w_j=\\frac{1}{N}\\sum_{i-1}^{N}g_{ji}$$\n",
    "$$\\theta_j=argmax_\\theta\\sum_{i-1}^{N}g_{ji}ln\\phi(\\theta;x)$$\n",
    "\n",
    "Выбираем «**скрытые переменные**», чтобы с их помощью максимизировать правдоподобие. E-шаг (expectation) — оцениваем скрытые переменные. M-шаг (maximization) – оцениваем $w_1, ......, w_k$ и $p_1(x), ......, p_k(x)$, фиксируем скрытые переменные.\n",
    "\n",
    "Можно выделить следующие преимущества алгоритма:\n",
    " - Эффективная обработка больших объемов данных (Big Data).\n",
    " - Мощная статистическая основа.\n",
    " - Устойчивость к шумам и пропускам в данных.\n",
    " - Возможность построения желаемого числа кластеров.\n",
    " - Быстрая сходимость при удачной инициализации.\n",
    "\n",
    "Недостатки алгоритма следующие:\n",
    " - При неудачной инициализации сходимость алгоритма может оказаться медленной.\n",
    " - Предположение о нормальности всех измерений данных не всегда выполняется.\n",
    " - Алгоритм иногда останавливается в локальном минимуме и не достигает глобального."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АГЛОМЕРАТИВНАЯ КЛАСТЕРИЗАЦИЯ\n",
    "\n",
    "**Иерархическая** кластеризация делится на две стратегии: **агломеративная** — снизу-вверх, объединяем точки в кластеры и **дивизионная** — сверху-вниз, разделяем один большой кластер на малые.\n",
    "\n",
    "1. Назначаем каждой точке свой кластер.\n",
    "2. Сортируем попарные расстояния между центрами кластеров по возрастанию.\n",
    "3. Берём пару ближайших кластеров, склеиваем их в один и пересчитываем центр кластера.\n",
    "4. Повторяем шаги 2-3 до тех пор, пока все данные не склеятся в один кластер.\n",
    "\n",
    "Визуально это выглядит следующим образом:\n",
    "\n",
    "<img src=\"../images/ml-4-4-1.png\" alt=\"Binary-cross-entropy\" width=\"500\" align=\"center\">\n",
    "\n",
    "**Расстояние между кластерами**:\n",
    "\n",
    "**Поиск** ближайших кластеров можно осуществлять с помощью разных методов объединения точек:\n",
    "\n",
    " - Расстояние ближнего соседа: $\\begin{equation}d\\left(C_{i}, C_{j}\\right)=\\min _{x_{i} \\in C_{i}, x_{j} \\in C_{j}}\\left\\|x_{i}-x_{j}\\right\\|\\end{equation}$\n",
    " - Расстояние дальнего соседа: $\\begin{equation}d\\left(C_{i}, C_{j}\\right)=\\max _{x_{i} \\in C_{i}, x_{j} \\in C_{j}}\\left\\|x_{i}-x_{j}\\right\\|\\end{equation}$\n",
    " - Групповое среднее расстояние: $\\begin{equation}d\\left(C_{i}, C_{j}\\right)=\\frac{1}{n_{i} n_{j}} \\sum_{x_{i} \\in C_{i}} \\sum_{x_{j} \\in C_{j}}\\left\\|x_{i}-x_{j}\\right\\|\\end{equation}$\n",
    " - Расстояние между центрами: $\\begin{equation}d\\left(C_{i}, C_{j}\\right)=\\left\\|\\mu_{i}-\\mu_{j}\\right\\|\\end{equation}$\n",
    " \n",
    "<img src=\"../images/ml-4-4-2.png\" alt=\"Binary-cross-entropy\" width=\"900\" align=\"center\">\n",
    "\n",
    "**Формула Ланса-Уильямса**: \n",
    "$$\\begin{equation}R(U \\cup V, S)=\\alpha_{U} R(U, S)+\\alpha_{V} R(V, S)+\\beta R(V, S)+\\gamma|R(U, S)-R(V, S)|\\end{equation}$$\n",
    "\n",
    "**Расстояние Уорда**: \n",
    "$$\\begin{equation}\\begin{array}{l} R^{w}(W, S)=\\frac{|S||W|}{|S|+|W|} \\rho^{2}\\left(\\sum_{w \\in W} \\frac{w}{|W|}, \\sum_{s \\in S} \\frac{s}{|S|}\\right) \\\\ \\alpha_{U}=\\frac{|S|+|U|}{|S|+|W|}, \\alpha_{V}=\\frac{|S|+|V|}{|S|+|W|}, \\beta=\\frac{-|S|}{|S|+|W|}, \\gamma=0 \\end{array}\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "Расшифровывается как Dense-based spatial clustering of applications with noise. Это основанная на плотности пространственная кластеризация для приложений с шумами.\n",
    "\n",
    "**Схема действия DBSCAN**\n",
    " 1. Случайно выбираем точку, которую не посещали. Окрестность точки извлекается с использованием расстояния $\\epsilon$.\n",
    " 2. Если в этой окрестности точек ≥ **minPoints**, тогда точка становится первой точкой в новом кластере. Иначе — помечаем точку как **шум**, она становится посещённой.\n",
    " 3. Точки из окрестности становятся частью кластера. Для каждой из них изучаем **окрестность**: если точек в окрестности < **minPoints**, то помечаем точку как граничную.\n",
    " 4. Повторяем пункты 2 и 3, пока не определим все точки в кластере.\n",
    " 5. Повторяем пункты 1–4, пока все точки не станут просмотренными.\n",
    " \n",
    "**Главная идея**:\n",
    " - Основные точки.\n",
    " - Граничные точки.\n",
    " - Шумовые точки.\n",
    "\n",
    "Достоинства алгоритма:\n",
    " - не требуется число кластеров;\n",
    " - определяем кластеры произвольной формы;\n",
    " - определяет шум, устойчив к выбросам.\n",
    "\n",
    "Недостатки алгоритма:\n",
    " - не может выделять кластеры, имеющие разную плотность;\n",
    " - результат зависит от используемой функции расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества\n",
    "\n",
    "Рассмотрим методы, с помощью которых можно оценить качество кластеризации. Обычно выделяют две большие группы методов:\n",
    " - Внешние — те, которые основаны на сравнении результата кластеризации с априори известным разделением на классы.\n",
    " - Внутренние — те, которые отображают качество кластеризации только по информации в данных.\n",
    "\n",
    "**ФУНКЦИОНАЛЫ КАЧЕСТВА**\n",
    "\n",
    "Существует очень много различных функционалов качества для кластеризации, но не существует наилучшего или оптимального. Каждый метод кластеризации можно рассматривать как алгоритм поиска оптимального значения некоторого функционала.\n",
    "\n",
    "**Среднее внутрикластерное расстояние**:\n",
    "\n",
    "$F_0=\\frac{\\sum [y_i=y_j]\\rho (x_i,x_j)}{\\sum [y_i=y_j]}$ — значение данного функционала должно быть как можно меньше, чтобы объекты располагались как можно ближе друг к другу внутри кластера.\n",
    "\n",
    "**Среднее межкластерное расстояние**:\n",
    "\n",
    "$F_1=\\frac{\\sum [y_i\\neq y_j]\\rho (x_i,x_j)}{\\sum [y_i\\neq y_j]}$ — значение этого функционала должно быть как можно больше, чтобы объекты из разных кластеров находились друг от друга как можно дальше.\n",
    "\n",
    "**Отношение этих двух расстояний**:\n",
    "\n",
    "$\\frac{F_0}{F_1}\\rightarrow min$ —  мы можем учитывать оба функционала, рассмотренные ранее( расстояние внутри кластера и между кластерами), и оптимизировать его. Естественно, нам нужно, чтобы оно было минимально (это будет достигаться, если расстояние между кластерами максимально, а внутри кластера — минимально).\n",
    "\n",
    "**Среднее расстояние до центра:**\n",
    "\n",
    "$\\phi_0=\\sum_{y\\in Y}\\frac{1}{|K_y|}\\sum_{i:y_i=y}\\rho^{2}(x_i,\\mu_y)$ — должно быть как можно меньше, чтобы объекты располагались максимально близко к центру кластера.\n",
    "\n",
    "**Сумма межкластерных расстояний:**\n",
    "\n",
    "$\\phi_1=\\sum_{y_i=y}\\rho^{2}(\\mu_i,\\mu)$ — должно быть максимальным, так как кластеры должны находиться друг от друга на как можно большем расстоянии.\n",
    "\n",
    "**Отношение этих двух расстояний:**\n",
    "\n",
    "$\\frac{\\phi_0}{\\phi_1}\\rightarrow min$\n",
    "\n",
    "**КОЭФФИЦИЕНТ СИЛУЭТА**\n",
    "\n",
    "Значение силуэта показывает, насколько объект похож на свой кластер по сравнению с другими кластерами.\n",
    "- $a$ — среднее расстояние от данного объекта до объектов из того же кластера.\n",
    "- $b$ — среднее расстояние от данного объекта до объектов из ближайшего кластера.\n",
    "\n",
    "$$S=\\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "Значение коэффициента находится в диапазоне от -1 до +1, где высокое значение указывает, что объект хорошо согласуется с кластером, которому он принадлежит, и плохо согласуется с «чужими» кластерами. Если у подавляющего большинства объектов этот коэффициент высокий, то можно считать кластеризацию достаточно качественной. Если же у большого числа объектов низкий или отрицательный коэффициент силуэта, то, возможно, кластеров слишком много, слишком мало или просто данные плохо поддаются разделению на кластеры.\n",
    "\n",
    "**Проверка наличия кластерной структуры**\n",
    " - Генерируем $p$ случайных точек из равномерного распределения.\n",
    " - Генерируем $p$ случайных точек из обучающей выборки.\n",
    "\n",
    " - Вычисляем величину: $\\begin{equation}H_{i n d}=\\frac{\\sum_{n} w_{i}}{\\sum_{n} q_{i}+\\sum_{n} w_{i}}\\end{equation}$\n",
    "\n",
    "Эта статистика является одним из **индикаторов тенденции к группированию**. Для её расчета создается B псевдо-наборов данных, сгенерированных случайным образом на основе распределения с тем же стандартным отклонением, что и оригинальный набор данных. Для каждого наблюдения i из n рассчитывается среднее расстояние до k ближайших соседей: w между реальными объектами и q между искусственными объектами и их ближайшими реальными соседями. Если статистика превышает значение 0.5, то мы оставляем нулевую гипотезу, которая заключается в том, что q и w подобны и группируемые объекты распределены однородно и случайно\n",
    "\n",
    "В случае, если статистика меньше чем 0.25, можно с 90% уверенностью отвергнуть нулевую гипотезу в пользу альтеративной, которая говорит о наличии тенденции к группировке данных.\n",
    "\n",
    "\n",
    "**ВНЕШНИЕ МЕТРИКИ**\n",
    "\n",
    "Если исходные данные размечены, то мы можем использовать метки объектов для измерения **качества кластеризации**.\n",
    "\n",
    "**Однородность** — кластер состоит только из объектов одного класса: \n",
    "$$h=1-\\frac{H(C|K)}{H(C)}$$\n",
    "\n",
    "В данной формуле $H(C|K)$ — энтропия класса при условии кластера, а $H(C)$ — энтропия класса\n",
    "\n",
    "То есть, максимальное значение однородность достигает в том случае, если в кластере объекты одного класса.\n",
    "\n",
    "**Полнота** — достигает максимальное значение в том случае, когда все объекты из класса принадлежат одному кластеру:\n",
    "$$c=1-\\frac{H(K|C)}{H(K)}$$\n",
    "\n",
    "В данной формуле $H(K|C)$ — энтропия кластера при условии класса, а $H(K)$  — энтропия кластера\n",
    "V-мера — среднее гармоническое однородности и полноты, то есть метрика,  объединяющая эти два показателя:\n",
    "$$u=2\\frac{hc}{h+c}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
